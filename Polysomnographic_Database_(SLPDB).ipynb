{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ucchwas/Polysomnographic-Database-Respiration-and-ECG-features/blob/main/Polysomnographic_Database_(SLPDB).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCJNIkPmBBnd",
        "outputId": "789e513b-6f12-4553-ce41-ed2b739eefab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wfdb\n",
            "  Downloading wfdb-4.1.2-py3-none-any.whl (159 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/160.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.7/160.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.0/160.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: SoundFile>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from wfdb) (0.12.1)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from wfdb) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.10.1 in /usr/local/lib/python3.10/dist-packages (from wfdb) (1.25.2)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from wfdb) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from wfdb) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wfdb) (1.11.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->wfdb) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->wfdb) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.8.1->wfdb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.8.1->wfdb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.8.1->wfdb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.8.1->wfdb) (2024.7.4)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from SoundFile>=0.10.0->wfdb) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->SoundFile>=0.10.0->wfdb) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->wfdb) (1.16.0)\n",
            "Installing collected packages: wfdb\n",
            "Successfully installed wfdb-4.1.2\n",
            "Collecting pyentrp\n",
            "  Downloading pyentrp-1.0.0-py3-none-any.whl (10 kB)\n",
            "Collecting numpy<3.0,>=1.26 (from pyentrp)\n",
            "  Downloading numpy-2.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.3/19.3 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, pyentrp\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "astropy 5.3.4 requires numpy<2,>=1.21, but you have numpy 2.0.0 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires numpy<2.0a0,>=1.23, but you have numpy 2.0.0 which is incompatible.\n",
            "cupy-cuda12x 12.2.0 requires numpy<1.27,>=1.20, but you have numpy 2.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires numpy<2,>=1, but you have numpy 2.0.0 which is incompatible.\n",
            "numba 0.58.1 requires numpy<1.27,>=1.22, but you have numpy 2.0.0 which is incompatible.\n",
            "rmm-cu12 24.4.0 requires numpy<2.0a0,>=1.23, but you have numpy 2.0.0 which is incompatible.\n",
            "scipy 1.11.4 requires numpy<1.28.0,>=1.21.6, but you have numpy 2.0.0 which is incompatible.\n",
            "tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 2.0.0 which is incompatible.\n",
            "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.0.0 which is incompatible.\n",
            "transformers 4.42.4 requires numpy<2.0,>=1.17, but you have numpy 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.0.0 pyentrp-1.0.0\n",
            "Collecting tsfel\n",
            "  Downloading tsfel-0.1.7-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ipython>=7.4.0 in /usr/local/lib/python3.10/dist-packages (from tsfel) (7.34.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from tsfel) (2.0.0)\n",
            "Requirement already satisfied: pandas>=1.5.3 in /usr/local/lib/python3.10/dist-packages (from tsfel) (2.0.3)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from tsfel) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.10/dist-packages (from tsfel) (1.11.4)\n",
            "Requirement already satisfied: setuptools>=47.1.1 in /usr/local/lib/python3.10/dist-packages (from tsfel) (67.7.2)\n",
            "Requirement already satisfied: statsmodels>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from tsfel) (0.14.2)\n",
            "Collecting jedi>=0.16 (from ipython>=7.4.0->tsfel)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=7.4.0->tsfel) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=7.4.0->tsfel) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.4.0->tsfel) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.4.0->tsfel) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=7.4.0->tsfel) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=7.4.0->tsfel) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=7.4.0->tsfel) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.4.0->tsfel) (4.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.3->tsfel) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.3->tsfel) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.3->tsfel) (2024.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->tsfel) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->tsfel) (3.5.0)\n",
            "Collecting numpy>=1.18.5 (from tsfel)\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.12.0->tsfel) (0.5.6)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.12.0->tsfel) (24.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=7.4.0->tsfel) (0.8.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.6->statsmodels>=0.12.0->tsfel) (1.16.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=7.4.0->tsfel) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.4.0->tsfel) (0.2.13)\n",
            "Installing collected packages: numpy, jedi, tsfel\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.0\n",
            "    Uninstalling numpy-2.0.0:\n",
            "      Successfully uninstalled numpy-2.0.0\n",
            "Successfully installed jedi-0.19.1 numpy-1.26.4 tsfel-0.1.7\n",
            "Collecting biosppy\n",
            "  Downloading biosppy-2.2.2-py2.py3-none-any.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.4/149.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: bidict in /usr/local/lib/python3.10/dist-packages (from biosppy) (0.23.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from biosppy) (3.9.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from biosppy) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biosppy) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from biosppy) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from biosppy) (1.11.4)\n",
            "Collecting shortuuid (from biosppy)\n",
            "  Downloading shortuuid-1.0.13-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from biosppy) (1.16.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from biosppy) (1.4.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from biosppy) (4.8.0.76)\n",
            "Requirement already satisfied: pywavelets in /usr/local/lib/python3.10/dist-packages (from biosppy) (1.6.0)\n",
            "Collecting mock (from biosppy)\n",
            "  Downloading mock-5.1.0-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->biosppy) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->biosppy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->biosppy) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->biosppy) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->biosppy) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->biosppy) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->biosppy) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->biosppy) (2.8.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->biosppy) (3.5.0)\n",
            "Installing collected packages: shortuuid, mock, biosppy\n",
            "Successfully installed biosppy-2.2.2 mock-5.1.0 shortuuid-1.0.13\n"
          ]
        }
      ],
      "source": [
        "!pip install wfdb\n",
        "import wfdb\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import entropy\n",
        "from scipy.signal import welch, find_peaks\n",
        "!pip install pyentrp\n",
        "from pyentrp import entropy as ent\n",
        "!pip install tsfel\n",
        "!pip install biosppy\n",
        "import tsfel\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxTXvJczorE0"
      },
      "outputs": [],
      "source": [
        "record_names = ['slp02a', 'slp02b', 'slp03', 'slp04', 'slp14', 'slp16', 'slp32', 'slp37', 'slp41', 'slp45', 'slp48']\n",
        "#'slp01a', 'slp01b', 'slp59', 'slp60', 'slp61', 'slp66', 'slp67x'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NOrU2tltJwM",
        "outputId": "3f025a1c-c9f2-41e3-e263-ce794adca83f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw data saved to 'SLPDB_Raw_Data_With_ECG.csv'\n"
          ]
        }
      ],
      "source": [
        "import wfdb\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize lists to store the signals and annotations\n",
        "resp_signals = []\n",
        "ecg_signals = []\n",
        "resp_annotations_list = []\n",
        "\n",
        "# Loop through each record name and read annotations\n",
        "for name in record_names:\n",
        "    annotations = wfdb.rdann(name, extension='st', pn_dir='slpdb')\n",
        "    resp_annotations_list.append(annotations)\n",
        "\n",
        "    record = wfdb.rdrecord(name, pn_dir='slpdb')\n",
        "    resp_signals.append(record.p_signal[:, 3].tolist())  # Assuming the respiration signal is the 4th channel\n",
        "    ecg_signals.append(record.p_signal[:, 0].tolist())   # Assuming the ECG signal is the 1st channel\n",
        "\n",
        "# Save the signals and annotations to a CSV file\n",
        "raw_data = {\n",
        "    'record_name': record_names,\n",
        "    'resp_signals': resp_signals,\n",
        "    'ecg_signals': ecg_signals,\n",
        "    'annotations_sample': [ann.sample.tolist() for ann in resp_annotations_list],\n",
        "    'annotations_aux_note': [ann.aux_note for ann in resp_annotations_list]\n",
        "}\n",
        "\n",
        "raw_df = pd.DataFrame(raw_data)\n",
        "raw_df.to_csv('SLPDB_Raw_Data_With_ECG.csv', index=False)\n",
        "\n",
        "print(\"Raw data saved to 'SLPDB_Raw_Data_With_ECG.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from collections import Counter\n",
        "\n",
        "# # Define the annotations of interest and their priority\n",
        "# priority_annotations = {\n",
        "#     \"CA\": \"3\",\n",
        "#     \"CAA\": \"3\",\n",
        "#     \"OA\": \"2\",\n",
        "#     \"X\": \"2\",\n",
        "#     \"H\": \"1\",\n",
        "#     \"HA\": \"1\",\n",
        "#     \"W\": \"0\"\n",
        "# }\n",
        "\n",
        "# # Initialize a counter for the sleep stages and apnea events\n",
        "# stage_counts = Counter()\n",
        "\n",
        "# # Sampling frequency and interval duration\n",
        "# fs = 250\n",
        "# interval_duration = 30\n",
        "\n",
        "# # Function to get the highest priority annotation in a segment\n",
        "# def get_priority_annotation(annotations, priority_annotations):\n",
        "#     for ann in priority_annotations:\n",
        "#         if ann in annotations:\n",
        "#             return priority_annotations[ann]\n",
        "#     return None\n",
        "\n",
        "# # Loop through each annotation list and process the segments\n",
        "# for resp_ann in resp_annotations_list:\n",
        "#     # Create an array to mark the annotations for each sample\n",
        "#     segment_annotations = [None] * (resp_ann.sample[-1] + 1)\n",
        "\n",
        "#     # Mark the annotations in the array\n",
        "#     for sample, symbol in zip(resp_ann.sample, resp_ann.aux_note):\n",
        "#         annotations = symbol.strip().split()\n",
        "#         for stage_key in annotations:\n",
        "#             if stage_key in priority_annotations:\n",
        "#                 segment_annotations[sample] = stage_key\n",
        "\n",
        "#     # Process the segments of 30-second intervals\n",
        "#     num_samples = len(segment_annotations)\n",
        "#     num_intervals = num_samples // (fs * interval_duration)\n",
        "\n",
        "#     for i in range(num_intervals):\n",
        "#         start = i * fs * interval_duration\n",
        "#         end = start + (fs * interval_duration)\n",
        "\n",
        "#         # Get annotations in this segment\n",
        "#         segment_ann = segment_annotations[start:end]\n",
        "\n",
        "#         # Remove None values\n",
        "#         segment_ann = [ann for ann in segment_ann if ann is not None]\n",
        "\n",
        "#         if segment_ann:\n",
        "#             # Get the highest priority annotation for this segment\n",
        "#             priority_ann = get_priority_annotation(segment_ann, priority_annotations)\n",
        "#             if priority_ann:\n",
        "#                 stage_counts[priority_ann] += 1\n",
        "\n",
        "# # Calculate the total number of intervals\n",
        "# total_samples = sum(resp_ann.sample[-1] for resp_ann in resp_annotations_list)\n",
        "# num_intervals = total_samples // (fs * interval_duration)\n",
        "\n",
        "# # Display results\n",
        "# print('Sleep Stage and Apnea Event Counts at 30-Second Intervals:')\n",
        "# for stage, count in stage_counts.items():\n",
        "#     print(f'{stage}: {count} occurrences in {num_intervals:.2f} intervals')\n",
        "\n",
        "# print('\\nPercentage of Each Stage:')\n",
        "# for stage, count in stage_counts.items():\n",
        "#     percentage = (count / num_intervals) * 100\n",
        "#     print(f'{stage}: {percentage:.2f}% of the time')"
      ],
      "metadata": {
        "id": "-F5B4px5lpK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.signal import find_peaks, welch, cwt, ricker\n",
        "from scipy.stats import entropy, kurtosis, skew\n",
        "from pywt import wavedec\n",
        "\n",
        "# Read the raw data from the CSV file\n",
        "raw_df = pd.read_csv('SLPDB_Raw_Data_With_ECG.csv')\n",
        "\n",
        "# Convert string representations back to lists\n",
        "raw_df['resp_signals'] = raw_df['resp_signals'].apply(eval)\n",
        "raw_df['ecg_signals'] = raw_df['ecg_signals'].apply(eval)\n",
        "raw_df['annotations_sample'] = raw_df['annotations_sample'].apply(eval)\n",
        "raw_df['annotations_aux_note'] = raw_df['annotations_aux_note'].apply(eval)\n",
        "\n",
        "# Debug: Print a few entries to verify data integrity\n",
        "print(raw_df.head())\n",
        "\n",
        "# Initialize empty list to store all features\n",
        "extracted_features_list = []\n",
        "\n",
        "# Define the annotations of interest and their priority\n",
        "priority_annotations = {\n",
        "    \"CA\": 3,\n",
        "    \"CAA\": 3,\n",
        "    \"OA\": 2,\n",
        "    \"X\": 2,\n",
        "    \"H\": 1,\n",
        "    \"HA\": 1,\n",
        "    \"W\": 0\n",
        "}\n",
        "\n",
        "def hjorth_parameters(segment):\n",
        "    \"\"\" Calculate Hjorth parameters: Activity, Mobility, and Complexity. \"\"\"\n",
        "    activity = np.var(segment)\n",
        "    diff = np.diff(segment)\n",
        "    mobility = np.sqrt(np.var(diff) / activity)\n",
        "    diff2 = np.diff(diff)\n",
        "    complexity = np.sqrt(np.var(diff2) / np.var(diff)) / mobility\n",
        "    return activity, mobility, complexity\n",
        "\n",
        "def wavelet_features(segment, wavelet='db1'):\n",
        "    \"\"\" Extract wavelet features from the signal. \"\"\"\n",
        "    coeffs = wavedec(segment, wavelet)\n",
        "    features = []\n",
        "    for coeff in coeffs:\n",
        "        features.extend([np.mean(coeff), np.std(coeff), skew(coeff), kurtosis(coeff)])\n",
        "    return features\n",
        "\n",
        "def get_priority_annotation(annotations, priority_annotations):\n",
        "    \"\"\" Get the highest priority annotation in a segment. \"\"\"\n",
        "    for ann in priority_annotations:\n",
        "        if ann in annotations:\n",
        "            return priority_annotations[ann]\n",
        "    return None"
      ],
      "metadata": {
        "id": "3YkjAjcAxwRa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ac8f276-f55c-4bad-e434-20199a0a3880"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  record_name                                       resp_signals  \\\n",
            "0      slp02a  [-0.03225806451612903, -0.016129032258064516, ...   \n",
            "1      slp02b  [-0.5645161290322581, -0.5645161290322581, -0....   \n",
            "2       slp03  [0.32304038004750596, 0.332541567695962, 0.327...   \n",
            "3       slp04  [0.048154093097913325, 0.04735152487961477, 0....   \n",
            "4       slp14  [0.051036682615629984, 0.04585326953748006, 0....   \n",
            "\n",
            "                                         ecg_signals  \\\n",
            "0  [-0.008, -0.008, -0.012, -0.008, -0.0, -0.0, 0...   \n",
            "1  [0.218, 0.218, 0.212, 0.214, 0.212, 0.2, 0.192...   \n",
            "2  [0.028, 0.026, -0.006, 0.006, -0.008, -0.054, ...   \n",
            "3  [0.416, 0.41, 0.406, 0.398, 0.4, 0.386, 0.37, ...   \n",
            "4  [0.04, 0.046, 0.05, 0.056, 0.058, 0.058, 0.072...   \n",
            "\n",
            "                                  annotations_sample  \\\n",
            "0  [1, 7500, 15000, 22500, 30000, 37500, 45000, 5...   \n",
            "1  [1, 7500, 15000, 22500, 30000, 37500, 45000, 5...   \n",
            "2  [1, 7500, 15000, 22500, 30000, 37500, 45000, 5...   \n",
            "3  [1, 7500, 15000, 22500, 30000, 37500, 45000, 5...   \n",
            "4  [45000, 52500, 60000, 67500, 75000, 82500, 900...   \n",
            "\n",
            "                                annotations_aux_note  \n",
            "0  [2, 2 L, 2, 2 LA, 2, 2 L, MT X, MT, 2 X, 2 X, ...  \n",
            "1  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2 L, 2...  \n",
            "2  [2 HA, 2 HA, 2 HA, 2 HA HA, 2 HA, 2 HA, 2 HA, ...  \n",
            "3  [W, 1 X, 1 X, 2 X, 2 X X, 2 X, 2 X, W X, 1 X, ...  \n",
            "4  [W, W, W, W, W, W, W, W, W, W, 1, W, W, W, W, ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through each row in the raw data\n",
        "for index, row in raw_df.iterrows():\n",
        "    record_name = row['record_name']\n",
        "    resp_signals = np.array(row['resp_signals'])\n",
        "    ecg_signals = np.array(row['ecg_signals'])\n",
        "    annotations_sample = np.array(row['annotations_sample'], dtype=int)\n",
        "    annotations_aux_note = row['annotations_aux_note']\n",
        "\n",
        "    # Create an array to mark the annotations for each sample\n",
        "    segment_annotations = [None] * (annotations_sample[-1] + 1)\n",
        "\n",
        "    # Mark the annotations in the array\n",
        "    for sample, symbol in zip(annotations_sample, annotations_aux_note):\n",
        "        annotations = symbol.strip().split()\n",
        "        for stage_key in annotations:\n",
        "            if stage_key in priority_annotations:\n",
        "                segment_annotations[sample] = stage_key\n",
        "\n",
        "    # Process the segments of 30-second intervals\n",
        "    num_samples = len(segment_annotations)\n",
        "    num_intervals = num_samples // (250 * 30)  # Assuming the sample rate is 250 Hz and intervals are 30 seconds\n",
        "\n",
        "    for i in range(num_intervals):\n",
        "        start = i * 250 * 30\n",
        "        end = start + (250 * 30)\n",
        "\n",
        "        # Get annotations in this segment\n",
        "        segment_ann = segment_annotations[start:end]\n",
        "\n",
        "        # Remove None values\n",
        "        segment_ann = [ann for ann in segment_ann if ann is not None]\n",
        "\n",
        "        # Check if segment contains only expected labels\n",
        "        if not segment_ann or any(ann not in priority_annotations for ann in segment_ann):\n",
        "            continue  # Skip segments with unexpected annotations\n",
        "\n",
        "        # Get the highest priority annotation for this segment\n",
        "        priority_ann = get_priority_annotation(segment_ann, priority_annotations)\n",
        "        if priority_ann is not None:\n",
        "            # Extract features from the respiratory segment\n",
        "            resp_segment = resp_signals[start:end]\n",
        "\n",
        "            # Ensure the segment is non-zero and normalized\n",
        "            if np.sum(resp_segment) == 0 or len(resp_segment) < 2:\n",
        "                continue\n",
        "\n",
        "            resp_segment = resp_segment - np.min(resp_segment)  # Shift to make all values positive\n",
        "            if np.max(resp_segment) != 0:\n",
        "                resp_segment = resp_segment / np.max(resp_segment)  # Normalize to max value 1\n",
        "\n",
        "            # Time domain features for respiratory segment\n",
        "            resp_mean = np.mean(resp_segment)\n",
        "            resp_std = np.std(resp_segment)\n",
        "            resp_skewness = skew(resp_segment)\n",
        "            resp_area_abs = np.sum(np.abs(resp_segment))\n",
        "            resp_kurt = kurtosis(resp_segment)\n",
        "            resp_min_value = np.min(resp_segment)\n",
        "            resp_max_value = np.max(resp_segment)\n",
        "            resp_rms = np.sqrt(np.mean(resp_segment ** 2))\n",
        "            resp_zcr = (np.diff(np.sign(resp_segment)) != 0).sum()\n",
        "            resp_signal_energy = np.sum(resp_segment ** 2)\n",
        "            resp_entropy_signal = entropy(resp_segment + 1e-10, base=2)\n",
        "\n",
        "            # Hjorth parameters for respiratory segment\n",
        "            resp_activity, resp_mobility, resp_complexity = hjorth_parameters(resp_segment)\n",
        "\n",
        "            # Wavelet features for respiratory segment\n",
        "            resp_wavelet_features = wavelet_features(resp_segment)\n",
        "\n",
        "            # Peak-related features using find_peaks for respiratory segment\n",
        "            resp_peaks, _ = find_peaks(resp_segment)\n",
        "            if len(resp_peaks) > 0:\n",
        "                resp_peak_heights = resp_segment[resp_peaks]\n",
        "                resp_mean_peak_height = np.mean(resp_peak_heights)\n",
        "                resp_std_peak_height = np.std(resp_peak_heights)\n",
        "                resp_skewness_peak_height = skew(resp_peak_heights)\n",
        "                resp_min_peak_height = np.min(resp_peak_heights)\n",
        "                resp_max_peak_height = np.max(resp_peak_heights)\n",
        "                resp_num_peaks = len(resp_peaks)\n",
        "                if resp_num_peaks > 1:\n",
        "                    resp_peak_distances = np.diff(resp_peaks)\n",
        "                    resp_mean_inter_peak_distance = np.mean(resp_peak_distances)\n",
        "                    resp_std_peak_distance = np.std(resp_peak_distances)\n",
        "                    resp_skewness_inter_peak_distance = skew(resp_peak_distances)\n",
        "                else:\n",
        "                    resp_mean_inter_peak_distance = resp_std_peak_distance = resp_skewness_inter_peak_distance = 0\n",
        "            else:\n",
        "                resp_mean_peak_height = resp_std_peak_height = resp_skewness_peak_height = 0\n",
        "                resp_min_peak_height = resp_max_peak_height = 0\n",
        "                resp_num_peaks = 0\n",
        "                resp_mean_inter_peak_distance = resp_std_peak_distance = resp_skewness_inter_peak_distance = 0\n",
        "            resp_sum_peak_heights = np.sum(resp_peak_heights) if len(resp_peaks) > 0 else 0\n",
        "            resp_peak_frequency = len(resp_segment) / np.sum(resp_segment > (np.max(resp_segment) / 2))\n",
        "\n",
        "            # Frequency domain features for respiratory segment\n",
        "            resp_fft_result = np.fft.fft(resp_segment)\n",
        "            resp_magnitude_spectrum = np.abs(resp_fft_result[:len(resp_fft_result)//2])  # Take the positive frequencies\n",
        "            resp_power_spectrum = resp_magnitude_spectrum ** 2\n",
        "            resp_dominant_frequency = np.argmax(resp_power_spectrum)\n",
        "            resp_total_power = np.sum(resp_power_spectrum)\n",
        "            resp_mean_frequency = np.sum(resp_magnitude_spectrum * np.arange(len(resp_magnitude_spectrum))) / np.sum(resp_magnitude_spectrum)\n",
        "            resp_central_frequency = np.argmax(resp_magnitude_spectrum[len(resp_magnitude_spectrum) // 2:]) + len(resp_magnitude_spectrum) // 2\n",
        "            resp_low_frequency_index = 0\n",
        "            resp_high_frequency_index = len(resp_power_spectrum) // 2\n",
        "            resp_band_power = np.sum(resp_power_spectrum[resp_low_frequency_index:resp_high_frequency_index])\n",
        "            resp_f, resp_pxx = welch(resp_segment, fs=250, nperseg=len(resp_segment)//2)\n",
        "            resp_spectral_centroid = np.sum(resp_f * resp_pxx) / np.sum(resp_pxx)\n",
        "            resp_spectral_spread = np.sqrt(np.sum(((resp_f - resp_spectral_centroid) ** 2) * resp_pxx) / np.sum(resp_pxx))\n",
        "            resp_spectral_skewness = np.sum(((resp_f - resp_spectral_centroid) ** 3) * resp_pxx) / (resp_spectral_spread ** 3)\n",
        "            resp_spectral_kurtosis = np.sum(((resp_f - resp_spectral_centroid) ** 4) * resp_pxx) / (resp_spectral_spread ** 4)\n",
        "\n",
        "            # Slope-related features for respiratory segment\n",
        "            resp_slopes = np.diff(resp_segment)\n",
        "            resp_mean_slope = np.mean(resp_slopes)\n",
        "            resp_std_slope = np.std(resp_slopes)\n",
        "\n",
        "            # Statistical Features for respiratory segment\n",
        "            resp_median = np.median(resp_segment)\n",
        "            resp_percentile_25 = np.percentile(resp_segment, 25)\n",
        "            resp_percentile_75 = np.percentile(resp_segment, 75)\n",
        "            resp_data_range = np.max(resp_segment) - np.min(resp_segment)\n",
        "\n",
        "            # Extract features from the ECG segment\n",
        "            ecg_segment = ecg_signals[start:end]\n",
        "\n",
        "            # Ensure the segment is non-zero and normalized\n",
        "            if np.sum(ecg_segment) == 0 or len(ecg_segment) < 2:\n",
        "                continue\n",
        "\n",
        "            ecg_segment = ecg_segment - np.min(ecg_segment)  # Shift to make all values positive\n",
        "            if np.max(ecg_segment) != 0:\n",
        "                ecg_segment = ecg_segment / np.max(ecg_segment)  # Normalize to max value 1\n",
        "\n",
        "            # Time domain features for ECG segment\n",
        "            ecg_mean = np.mean(ecg_segment)\n",
        "            ecg_std = np.std(ecg_segment)\n",
        "            ecg_skewness = skew(ecg_segment)\n",
        "            ecg_area_abs = np.sum(np.abs(ecg_segment))\n",
        "            ecg_kurt = kurtosis(ecg_segment)\n",
        "            ecg_min_value = np.min(ecg_segment)\n",
        "            ecg_max_value = np.max(ecg_segment)\n",
        "            ecg_rms = np.sqrt(np.mean(ecg_segment ** 2))\n",
        "            ecg_zcr = (np.diff(np.sign(ecg_segment)) != 0).sum()\n",
        "            ecg_signal_energy = np.sum(ecg_segment ** 2)\n",
        "            ecg_entropy_signal = entropy(ecg_segment + 1e-10, base=2)\n",
        "\n",
        "            # Hjorth parameters for ECG segment\n",
        "            ecg_activity, ecg_mobility, ecg_complexity = hjorth_parameters(ecg_segment)\n",
        "\n",
        "            # Wavelet features for ECG segment\n",
        "            ecg_wavelet_features = wavelet_features(ecg_segment)\n",
        "\n",
        "            # Peak-related features using find_peaks for ECG segment\n",
        "            ecg_peaks, _ = find_peaks(ecg_segment)\n",
        "            if len(ecg_peaks) > 0:\n",
        "                ecg_peak_heights = ecg_segment[ecg_peaks]\n",
        "                ecg_mean_peak_height = np.mean(ecg_peak_heights)\n",
        "                ecg_std_peak_height = np.std(ecg_peak_heights)\n",
        "                ecg_skewness_peak_height = skew(ecg_peak_heights)\n",
        "                ecg_min_peak_height = np.min(ecg_peak_heights)\n",
        "                ecg_max_peak_height = np.max(ecg_peak_heights)\n",
        "                ecg_num_peaks = len(ecg_peaks)\n",
        "                if ecg_num_peaks > 1:\n",
        "                    ecg_peak_distances = np.diff(ecg_peaks)\n",
        "                    ecg_mean_inter_peak_distance = np.mean(ecg_peak_distances)\n",
        "                    ecg_std_peak_distance = np.std(ecg_peak_distances)\n",
        "                    ecg_skewness_inter_peak_distance = skew(ecg_peak_distances)\n",
        "                else:\n",
        "                    ecg_mean_inter_peak_distance = ecg_std_peak_distance = ecg_skewness_inter_peak_distance = 0\n",
        "            else:\n",
        "                ecg_mean_peak_height = ecg_std_peak_height = ecg_skewness_peak_height = 0\n",
        "                ecg_min_peak_height = ecg_max_peak_height = 0\n",
        "                ecg_num_peaks = 0\n",
        "                ecg_mean_inter_peak_distance = ecg_std_peak_distance = ecg_skewness_inter_peak_distance = 0\n",
        "            ecg_sum_peak_heights = np.sum(ecg_peak_heights) if len(ecg_peaks) > 0 else 0\n",
        "            ecg_peak_frequency = len(ecg_segment) / np.sum(ecg_segment > (np.max(ecg_segment) / 2))\n",
        "\n",
        "            # Frequency domain features for ECG segment\n",
        "            ecg_fft_result = np.fft.fft(ecg_segment)\n",
        "            ecg_magnitude_spectrum = np.abs(ecg_fft_result[:len(ecg_fft_result)//2])  # Take the positive frequencies\n",
        "            ecg_power_spectrum = ecg_magnitude_spectrum ** 2\n",
        "            ecg_dominant_frequency = np.argmax(ecg_power_spectrum)\n",
        "            ecg_total_power = np.sum(ecg_power_spectrum)\n",
        "            ecg_mean_frequency = np.sum(ecg_magnitude_spectrum * np.arange(len(ecg_magnitude_spectrum))) / np.sum(ecg_magnitude_spectrum)\n",
        "            ecg_central_frequency = np.argmax(ecg_magnitude_spectrum[len(ecg_magnitude_spectrum) // 2:]) + len(ecg_magnitude_spectrum) // 2\n",
        "            ecg_low_frequency_index = 0\n",
        "            ecg_high_frequency_index = len(ecg_power_spectrum) // 2\n",
        "            ecg_band_power = np.sum(ecg_power_spectrum[ecg_low_frequency_index:ecg_high_frequency_index])\n",
        "            ecg_f, ecg_pxx = welch(ecg_segment, fs=250, nperseg=len(ecg_segment)//2)\n",
        "            ecg_spectral_centroid = np.sum(ecg_f * ecg_pxx) / np.sum(ecg_pxx)\n",
        "            ecg_spectral_spread = np.sqrt(np.sum(((ecg_f - ecg_spectral_centroid) ** 2) * ecg_pxx) / np.sum(ecg_pxx))\n",
        "            ecg_spectral_skewness = np.sum(((ecg_f - ecg_spectral_centroid) ** 3) * ecg_pxx) / (ecg_spectral_spread ** 3)\n",
        "            ecg_spectral_kurtosis = np.sum(((ecg_f - ecg_spectral_centroid) ** 4) * ecg_pxx) / (ecg_spectral_spread ** 4)\n",
        "\n",
        "            # Slope-related features for ECG segment\n",
        "            ecg_slopes = np.diff(ecg_segment)\n",
        "            ecg_mean_slope = np.mean(ecg_slopes)\n",
        "            ecg_std_slope = np.std(ecg_slopes)\n",
        "\n",
        "            # Statistical Features for ECG segment\n",
        "            ecg_median = np.median(ecg_segment)\n",
        "            ecg_percentile_25 = np.percentile(ecg_segment, 25)\n",
        "            ecg_percentile_75 = np.percentile(ecg_segment, 75)\n",
        "            ecg_data_range = np.max(ecg_segment) - np.min(ecg_segment)\n",
        "\n",
        "            # Store features along with label\n",
        "            features = [\n",
        "                # Respiratory features\n",
        "                resp_mean, resp_std, resp_skewness, resp_area_abs, resp_kurt, resp_min_value, resp_max_value,\n",
        "                resp_rms, resp_zcr, resp_signal_energy, resp_entropy_signal,\n",
        "                resp_mean_peak_height, resp_std_peak_height, resp_skewness_peak_height,\n",
        "                resp_num_peaks, resp_mean_inter_peak_distance, resp_std_peak_distance, resp_skewness_inter_peak_distance,\n",
        "                resp_sum_peak_heights, resp_peak_frequency, resp_max_peak_height, resp_min_peak_height,\n",
        "                resp_dominant_frequency, resp_central_frequency, resp_band_power,\n",
        "                resp_spectral_centroid, resp_spectral_spread, resp_spectral_skewness, resp_spectral_kurtosis,\n",
        "                resp_mean_slope, resp_std_slope,\n",
        "                resp_activity, resp_mobility, resp_complexity,\n",
        "                resp_median, resp_percentile_25, resp_percentile_75, resp_data_range,\n",
        "                *resp_wavelet_features,\n",
        "\n",
        "                # ECG features\n",
        "                ecg_mean, ecg_std, ecg_skewness, ecg_area_abs, ecg_kurt, ecg_min_value, ecg_max_value,\n",
        "                ecg_rms, ecg_zcr, ecg_signal_energy, ecg_entropy_signal,\n",
        "                ecg_mean_peak_height, ecg_std_peak_height, ecg_skewness_peak_height,\n",
        "                ecg_num_peaks, ecg_mean_inter_peak_distance, ecg_std_peak_distance, ecg_skewness_inter_peak_distance,\n",
        "                ecg_sum_peak_heights, ecg_peak_frequency, ecg_max_peak_height, ecg_min_peak_height,\n",
        "                ecg_dominant_frequency, ecg_central_frequency, ecg_band_power,\n",
        "                ecg_spectral_centroid, ecg_spectral_spread, ecg_spectral_skewness, ecg_spectral_kurtosis,\n",
        "                ecg_mean_slope, ecg_std_slope,\n",
        "                ecg_activity, ecg_mobility, ecg_complexity,\n",
        "                ecg_median, ecg_percentile_25, ecg_percentile_75, ecg_data_range,\n",
        "                *ecg_wavelet_features,\n",
        "\n",
        "                # Label\n",
        "                priority_ann\n",
        "            ]\n",
        "\n",
        "            extracted_features_list.append(features)\n",
        "\n",
        "# Convert the list of extracted features into a DataFrame\n",
        "df = pd.DataFrame(extracted_features_list)\n",
        "\n",
        "# Fill NaN values with 0\n",
        "df = df.fillna(0)\n",
        "\n",
        "# Define column names\n",
        "resp_time_domain_features = ['resp_mean', 'resp_std', 'resp_skewness', 'resp_area_abs', 'resp_kurt', 'resp_min_value', 'resp_max_value', 'resp_rms', 'resp_zcr', 'resp_signal_energy', 'resp_entropy_signal']\n",
        "resp_peak_related_features = ['resp_mean_peak_height', 'resp_std_peak_height', 'resp_skewness_peak_height', 'resp_num_peaks', 'resp_mean_inter_peak_distance', 'resp_std_peak_distance', 'resp_skewness_inter_peak_distance', 'resp_sum_peak_heights', 'resp_peak_frequency', 'resp_max_peak_height', 'resp_min_peak_height']\n",
        "resp_frequency_domain_features = ['resp_dominant_frequency', 'resp_central_frequency', 'resp_band_power', 'resp_spectral_centroid', 'resp_spectral_spread', 'resp_spectral_skewness', 'resp_spectral_kurtosis']\n",
        "resp_slope_features = ['resp_mean_slope', 'resp_std_slope']\n",
        "resp_hjorth_features = ['resp_activity', 'resp_mobility', 'resp_complexity']\n",
        "resp_statistical_features = ['resp_median', 'resp_percentile_25', 'resp_percentile_75', 'resp_data_range']\n",
        "resp_wavelet_features_names = [f'resp_wavelet_{i}' for i in range(4 * len(wavedec(resp_segment, 'db1')))]  # 4 features per wavelet coefficient set\n",
        "\n",
        "ecg_time_domain_features = ['ecg_mean', 'ecg_std', 'ecg_skewness', 'ecg_area_abs', 'ecg_kurt', 'ecg_min_value', 'ecg_max_value', 'ecg_rms', 'ecg_zcr', 'ecg_signal_energy', 'ecg_entropy_signal']\n",
        "ecg_peak_related_features = ['ecg_mean_peak_height', 'ecg_std_peak_height', 'ecg_skewness_peak_height', 'ecg_num_peaks', 'ecg_mean_inter_peak_distance', 'ecg_std_peak_distance', 'ecg_skewness_inter_peak_distance', 'ecg_sum_peak_heights', 'ecg_peak_frequency', 'ecg_max_peak_height', 'ecg_min_peak_height']\n",
        "ecg_frequency_domain_features = ['ecg_dominant_frequency', 'ecg_central_frequency', 'ecg_band_power', 'ecg_spectral_centroid', 'ecg_spectral_spread', 'ecg_spectral_skewness', 'ecg_spectral_kurtosis']\n",
        "ecg_slope_features = ['ecg_mean_slope', 'ecg_std_slope']\n",
        "ecg_hjorth_features = ['ecg_activity', 'ecg_mobility', 'ecg_complexity']\n",
        "ecg_statistical_features = ['ecg_median', 'ecg_percentile_25', 'ecg_percentile_75', 'ecg_data_range']\n",
        "ecg_wavelet_features_names = [f'ecg_wavelet_{i}' for i in range(4 * len(wavedec(ecg_segment, 'db1')))]  # 4 features per wavelet coefficient set\n",
        "\n",
        "label_column = ['label']\n",
        "\n",
        "column_names = (resp_time_domain_features + resp_peak_related_features + resp_frequency_domain_features +\n",
        "                resp_slope_features + resp_hjorth_features + resp_statistical_features + resp_wavelet_features_names +\n",
        "                ecg_time_domain_features + ecg_peak_related_features + ecg_frequency_domain_features +\n",
        "                ecg_slope_features + ecg_hjorth_features + ecg_statistical_features + ecg_wavelet_features_names +\n",
        "                label_column)\n",
        "\n",
        "# Rename the columns of the DataFrame\n",
        "df.columns = column_names\n",
        "\n",
        "# Save the DataFrame to a CSV file with labeled columns and excluding the index\n",
        "df.to_csv('SLPDB_Resp_ECG_Features_Enhanced.csv', index=False)\n",
        "\n",
        "# Print the shape of the DataFrame\n",
        "print(\"Shape of the DataFrame:\", df.shape)"
      ],
      "metadata": {
        "id": "o5L8CqWJx7FX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69d47b0d-ac09-46d3-e353-1eef167eba6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the DataFrame: (3744, 181)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from scipy.signal import find_peaks, welch\n",
        "# from scipy.stats import entropy, kurtosis, skew\n",
        "\n",
        "# # Read the raw data from the CSV file\n",
        "# raw_df = pd.read_csv('SLPDB_Raw_Data.csv')\n",
        "\n",
        "# # Convert string representations back to lists\n",
        "# raw_df['resp_signals'] = raw_df['resp_signals'].apply(eval)\n",
        "# raw_df['annotations_sample'] = raw_df['annotations_sample'].apply(eval)\n",
        "# raw_df['annotations_aux_note'] = raw_df['annotations_aux_note'].apply(eval)\n",
        "\n",
        "# # Debug: Print a few entries to verify data integrity\n",
        "# print(raw_df.head())\n",
        "\n",
        "# # Initialize empty list to store all features\n",
        "# extracted_features_list = []\n",
        "\n",
        "# # Define the annotations of interest and their priority\n",
        "# priority_annotations = {\n",
        "#     \"CA\": 3,\n",
        "#     \"CAA\": 3,\n",
        "#     \"OA\": 2,\n",
        "#     \"X\": 2,\n",
        "#     \"H\": 1,\n",
        "#     \"HA\": 1,\n",
        "#     \"W\": 0\n",
        "# }\n",
        "\n",
        "# def hjorth_parameters(segment):\n",
        "#     \"\"\" Calculate Hjorth parameters: Activity, Mobility, and Complexity. \"\"\"\n",
        "#     activity = np.var(segment)\n",
        "#     diff = np.diff(segment)\n",
        "#     mobility = np.sqrt(np.var(diff) / activity)\n",
        "#     diff2 = np.diff(diff)\n",
        "#     complexity = np.sqrt(np.var(diff2) / np.var(diff)) / mobility\n",
        "#     return activity, mobility, complexity\n",
        "\n",
        "# # Function to get the highest priority annotation in a segment\n",
        "# def get_priority_annotation(annotations, priority_annotations):\n",
        "#     for ann in priority_annotations:\n",
        "#         if ann in annotations:\n",
        "#             return priority_annotations[ann]\n",
        "#     return None\n",
        "\n",
        "# # Loop through each row in the raw data\n",
        "# for index, row in raw_df.iterrows():\n",
        "#     record_name = row['record_name']\n",
        "#     resp_signals = np.array(row['resp_signals'])\n",
        "#     annotations_sample = np.array(row['annotations_sample'], dtype=int)\n",
        "#     annotations_aux_note = row['annotations_aux_note']\n",
        "\n",
        "#     # Create an array to mark the annotations for each sample\n",
        "#     segment_annotations = [None] * (annotations_sample[-1] + 1)\n",
        "\n",
        "#     # Mark the annotations in the array\n",
        "#     for sample, symbol in zip(annotations_sample, annotations_aux_note):\n",
        "#         annotations = symbol.strip().split()\n",
        "#         for stage_key in annotations:\n",
        "#             if stage_key in priority_annotations:\n",
        "#                 segment_annotations[sample] = stage_key\n",
        "\n",
        "#     # Process the segments of 30-second intervals\n",
        "#     num_samples = len(segment_annotations)\n",
        "#     num_intervals = num_samples // (250 * 30)  # Assuming the sample rate is 250 Hz and intervals are 30 seconds\n",
        "\n",
        "#     for i in range(num_intervals):\n",
        "#         start = i * 250 * 30\n",
        "#         end = start + (250 * 30)\n",
        "\n",
        "#         # Get annotations in this segment\n",
        "#         segment_ann = segment_annotations[start:end]\n",
        "\n",
        "#         # Remove None values\n",
        "#         segment_ann = [ann for ann in segment_ann if ann is not None]\n",
        "\n",
        "#         # Check if segment contains only expected labels\n",
        "#         if not segment_ann or any(ann not in priority_annotations for ann in segment_ann):\n",
        "#             continue  # Skip segments with unexpected annotations\n",
        "\n",
        "#         # Get the highest priority annotation for this segment\n",
        "#         priority_ann = get_priority_annotation(segment_ann, priority_annotations)\n",
        "#         if priority_ann is not None:\n",
        "#             # Extract features from the segment\n",
        "#             segment = resp_signals[start:end]\n",
        "\n",
        "#             # Ensure the segment is non-zero and normalized\n",
        "#             if np.sum(segment) == 0 or len(segment) < 2:\n",
        "#                 continue\n",
        "\n",
        "#             segment = segment - np.min(segment)  # Shift to make all values positive\n",
        "#             if np.max(segment) != 0:\n",
        "#                 segment = segment / np.max(segment)  # Normalize to max value 1\n",
        "\n",
        "#             # Time domain features\n",
        "#             mean = np.mean(segment)\n",
        "#             std = np.std(segment)\n",
        "#             skewness = skew(segment)\n",
        "#             area_abs = np.sum(np.abs(segment))\n",
        "#             kurt = kurtosis(segment)\n",
        "#             min_value = np.min(segment)\n",
        "#             max_value = np.max(segment)\n",
        "#             rms = np.sqrt(np.mean(segment ** 2))\n",
        "#             zcr = (np.diff(np.sign(segment)) != 0).sum()\n",
        "#             signal_energy = np.sum(segment ** 2)\n",
        "#             entropy_signal = entropy(segment + 1e-10, base=2)\n",
        "\n",
        "#             # Hjorth parameters\n",
        "#             activity, mobility, complexity = hjorth_parameters(segment)\n",
        "\n",
        "#             # Peak-related features using find_peaks\n",
        "#             peaks, _ = find_peaks(segment)\n",
        "#             if len(peaks) > 0:\n",
        "#                 peak_heights = segment[peaks]\n",
        "#                 mean_peak_height = np.mean(peak_heights)\n",
        "#                 std_peak_height = np.std(peak_heights)\n",
        "#                 skewness_peak_height = skew(peak_heights)\n",
        "#                 min_peak_height = np.min(peak_heights)\n",
        "#                 max_peak_height = np.max(peak_heights)\n",
        "#                 num_peaks = len(peaks)\n",
        "#                 if num_peaks > 1:\n",
        "#                     peak_distances = np.diff(peaks)\n",
        "#                     mean_inter_peak_distance = np.mean(peak_distances)\n",
        "#                     std_peak_distance = np.std(peak_distances)\n",
        "#                     skewness_inter_peak_distance = skew(peak_distances)\n",
        "#                 else:\n",
        "#                     mean_inter_peak_distance = std_peak_distance = skewness_inter_peak_distance = 0\n",
        "#             else:\n",
        "#                 mean_peak_height = std_peak_height = skewness_peak_height = 0\n",
        "#                 min_peak_height = max_peak_height = 0\n",
        "#                 num_peaks = 0\n",
        "#                 mean_inter_peak_distance = std_peak_distance = skewness_inter_peak_distance = 0\n",
        "#             sum_peak_heights = np.sum(peak_heights) if len(peaks) > 0 else 0\n",
        "#             peak_frequency = len(segment) / np.sum(segment > (np.max(segment) / 2))\n",
        "\n",
        "#             # Frequency domain features\n",
        "#             fft_result = np.fft.fft(segment)\n",
        "#             magnitude_spectrum = np.abs(fft_result[:len(fft_result)//2])  # Take the positive frequencies\n",
        "#             power_spectrum = magnitude_spectrum ** 2\n",
        "#             dominant_frequency = np.argmax(power_spectrum)\n",
        "#             total_power = np.sum(power_spectrum)\n",
        "#             mean_frequency = np.sum(magnitude_spectrum * np.arange(len(magnitude_spectrum))) / np.sum(magnitude_spectrum)\n",
        "#             central_frequency = np.argmax(magnitude_spectrum[len(magnitude_spectrum) // 2:]) + len(magnitude_spectrum) // 2\n",
        "#             low_frequency_index = 0\n",
        "#             high_frequency_index = len(power_spectrum) // 2\n",
        "#             band_power = np.sum(power_spectrum[low_frequency_index:high_frequency_index])\n",
        "#             f, pxx = welch(segment, fs=250, nperseg=len(segment)//2)\n",
        "#             spectral_centroid = np.sum(f * pxx) / np.sum(pxx)\n",
        "#             spectral_spread = np.sqrt(np.sum(((f - spectral_centroid) ** 2) * pxx) / np.sum(pxx))\n",
        "#             spectral_skewness = np.sum(((f - spectral_centroid) ** 3) * pxx) / (spectral_spread ** 3)\n",
        "#             spectral_kurtosis = np.sum(((f - spectral_centroid) ** 4) * pxx) / (spectral_spread ** 4)\n",
        "\n",
        "#             # Slope-related features\n",
        "#             slopes = np.diff(segment)\n",
        "#             mean_slope = np.mean(slopes)\n",
        "#             std_slope = np.std(slopes)\n",
        "\n",
        "#             # Statistical Features\n",
        "#             median = np.median(segment)\n",
        "#             percentile_25 = np.percentile(segment, 25)\n",
        "#             percentile_75 = np.percentile(segment, 75)\n",
        "#             data_range = np.max(segment) - np.min(segment)\n",
        "\n",
        "#             # Store features along with label\n",
        "#             features = [mean, std, skewness, area_abs, kurt, min_value, max_value,\n",
        "#                         rms, zcr, signal_energy, entropy_signal,\n",
        "#                         mean_peak_height, std_peak_height, skewness_peak_height,\n",
        "#                         num_peaks, mean_inter_peak_distance, std_peak_distance, skewness_inter_peak_distance,\n",
        "#                         sum_peak_heights, peak_frequency, max_peak_height, min_peak_height,\n",
        "#                         dominant_frequency, central_frequency, band_power,\n",
        "#                         spectral_centroid, spectral_spread, spectral_skewness, spectral_kurtosis,\n",
        "#                         mean_slope, std_slope,\n",
        "#                         activity, mobility, complexity,\n",
        "#                         median, percentile_25, percentile_75, data_range,\n",
        "#                         priority_ann]\n",
        "\n",
        "#             extracted_features_list.append(features)\n",
        "\n",
        "# # Convert the list of extracted features into a DataFrame\n",
        "# df = pd.DataFrame(extracted_features_list)\n",
        "\n",
        "# # Fill NaN values with 0\n",
        "# df = df.fillna(0)\n",
        "\n",
        "# # Define column names\n",
        "# time_domain_features = ['mean', 'std', 'skewness', 'area_abs', 'kurt', 'min_value', 'max_value', 'rms', 'zcr', 'signal_energy', 'entropy_signal']\n",
        "# peak_related_features = ['mean_peak_height', 'std_peak_height', 'skewness_peak_height', 'num_peaks', 'mean_inter_peak_distance', 'std_peak_distance', 'skewness_inter_peak_distance', 'sum_peak_heights', 'peak_frequency', 'max_peak_height', 'min_peak_height']\n",
        "# frequency_domain_features = ['dominant_frequency', 'central_frequency', 'band_power', 'spectral_centroid', 'spectral_spread', 'spectral_skewness', 'spectral_kurtosis']\n",
        "# slope_features = ['mean_slope', 'std_slope']\n",
        "# hjorth_features = ['activity', 'mobility', 'complexity']\n",
        "# statistical_features = ['median', 'percentile_25', 'percentile_75', 'data_range']\n",
        "# label_column = ['label']\n",
        "\n",
        "# column_names = time_domain_features + peak_related_features + frequency_domain_features + slope_features + hjorth_features + statistical_features + label_column\n",
        "\n",
        "# # Rename the columns of the DataFrame\n",
        "# df.columns = column_names\n",
        "\n",
        "# # Save the DataFrame to a CSV file with labeled columns and excluding the index\n",
        "# df.to_csv('SLPDB_Resp_Features_Enhanced.csv', index=False)\n",
        "\n",
        "# # Print the shape of the DataFrame\n",
        "# print(\"Shape of the DataFrame:\", df.shape)"
      ],
      "metadata": {
        "id": "zewjlb5Vt4tX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vln4utYFGgsP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4811cfb2-5f71-4f72-ad4b-fc45f5679000"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape after dropping NaNs: (3744, 181)\n",
            "Top 79 features saved to 'SLPDB_Top_79_Features_PCA.csv'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('SLPDB_Resp_ECG_Features_Enhanced.csv')\n",
        "\n",
        "# Convert all feature columns to numeric, handling errors by coercing to NaN\n",
        "for col in df.columns:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "# Drop rows with NaN values if any\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Check the shape of the dataframe after dropping NaN values\n",
        "print(\"Shape after dropping NaNs:\", df.shape)\n",
        "\n",
        "# Separate features and labels\n",
        "X = df.drop(columns=['label'])\n",
        "y = df['label']\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA to preserve 95% of the variance\n",
        "pca = PCA(n_components=0.95)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Get the number of components selected\n",
        "n_components_selected = pca.n_components_\n",
        "\n",
        "# Get the original feature names that contribute most to each principal component\n",
        "pca_components = pca.components_\n",
        "top_features_names = []\n",
        "for i in range(n_components_selected):\n",
        "    component = pca_components[i]\n",
        "    top_feature_idx = np.argmax(np.abs(component))\n",
        "    top_feature_name = X.columns[top_feature_idx]\n",
        "    top_features_names.append(top_feature_name)\n",
        "\n",
        "# Create a new DataFrame with the selected top features\n",
        "top_features_df = pd.DataFrame(X_pca, columns=top_features_names)\n",
        "\n",
        "# Add the label column\n",
        "top_features_df['label'] = y.values\n",
        "\n",
        "# Sort the DataFrame by the label column\n",
        "top_features_df_sorted = top_features_df.sort_values(by='label')\n",
        "\n",
        "# Save the new DataFrame to a CSV file\n",
        "top_features_df_sorted.to_csv(f'SLPDB_Top_{n_components_selected}_Features_PCA.csv', index=False)\n",
        "\n",
        "print(f\"Top {n_components_selected} features saved to 'SLPDB_Top_{n_components_selected}_Features_PCA.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfWuKtKkBnCW"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Load the DataFrame containing the selected features and labels\n",
        "df_selected_features = pd.read_csv('SLPDB_Top_79_Features_PCA.csv')\n",
        "\n",
        "# Separate features and labels\n",
        "X = df_selected_features.drop(columns=['label'])\n",
        "y = df_selected_features['label']\n",
        "\n",
        "# Split the dataset randomly\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert scaled features back to DataFrame\n",
        "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
        "\n",
        "# Create DataFrames for training and testing sets\n",
        "df_train = pd.concat([X_train_scaled_df, y_train.reset_index(drop=True)], axis=1)\n",
        "df_test = pd.concat([X_test_scaled_df, y_test.reset_index(drop=True)], axis=1)\n",
        "\n",
        "# Save the training and testing DataFrames to CSV files\n",
        "df_train.to_csv('features_train.csv', index=False)\n",
        "df_test.to_csv('features_test.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMRnwriKPPwvsEjeizHexVu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}